import os
import json
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
# Flask ä¼ºæœå™¨
app = Flask(__name__)
CORS(app)

# æœ¬åœ° LLM API ä½ç½®
LLM_API_URL = "http://127.0.0.1:1234/v1/chat/completions"

# é è¨­æ¨¡å‹åç¨±ï¼ˆè«‹ç¢ºèªä½ çš„æ¨¡å‹åç¨±æ˜¯å¦æ­£ç¢ºï¼‰
MODEL_NAME = "hermes-3-llama-3.2-3b"

@app.route("/generate", methods=["POST"])
def generate_prompt():
    """æ¥æ”¶ç”¨æˆ¶è«‹æ±‚ï¼Œä¸¦è¿”å› AI ç”Ÿæˆçš„ Prompt"""
    data = request.json
    user_input = data.get("input", "").strip()

    if not user_input:
        return jsonify({"error": "è«‹æä¾›æœ‰æ•ˆçš„è¼¸å…¥"}), 400

    print(f"ğŸ‘¤ ä½¿ç”¨è€…è¼¸å…¥ï¼š{user_input}")

    # **System è¨Šæ¯ - è®“ LLM æˆç‚ºå°ˆæ¥­ Prompt ç”Ÿæˆå™¨**
    system_message = {
        "role": "system",
        "content": (
            "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å‰µæ„ç™¼æƒ³ç”Ÿæˆå™¨ï¼Œè² è²¬ç‚ºä½¿ç”¨è€…çš„è¼¸å…¥åšå‡ºæ¥µå…·å‰µæ„èˆ‡å„æœ‰å·®ç•°çš„é»å­ï¼Œä½†é‚„æ˜¯è¦ç›¡å¯èƒ½ç¬¦åˆç¾å¯¦ã€‚\n"
            "<think>ä½ éœ€è¦è€ƒæ…®ä½¿ç”¨è€…çš„å®Œæ•´è¼¸å…¥ï¼Œç”Ÿæˆå°æ–¼è©²å®Œæ•´è¼¸å…¥åŒ…æ‹¬å¤šç¨®ä¸åŒé¢å‘èˆ‡ç™¼æƒ³çš„å›ç­”ï¼Œå¯ä»¥æ˜¯å°ˆæ¥­çš„ç­”è¦†ã€æŠ½è±¡çš„æè¿°ï¼ŒæŠ‘æˆ–æ˜¯æœ‰è¶£çš„æ•…äº‹ã€‚</think>\n"
            "å„å›ç­”ä¹‹é–“çš„å·®ç•°æ€§è¶Šé è¶Šå¥½ï¼Œæœ€å¥½æ˜¯å®Œå…¨ä¸åŒæ€§è³ªèˆ‡æ„æ¶µçš„ç­”è¦†ã€‚\n"
            "<think>è«‹è‡³å¤šç”Ÿæˆ4åˆ°5å€‹ä¾‹å­ï¼Œä¸¦ä¸”æ¯ä¸€é»éƒ½ç°¡çŸ­èªªæ˜ï¼Œæ¯é»æ¿ƒç¸®æ–¼80å­—å…§ï¼Œç”¨ç¹é«”ä¸­æ–‡å­—å›ç­”ã€‚</think>"
        )
    }

    # å»ºç«‹ LLM è«‹æ±‚æ ¼å¼
    llm_payload = {
        "model": MODEL_NAME,
        "messages": [
            system_message,
            {"role": "user", "content": user_input}
        ],
        "temperature": 0.7,
        "max_tokens": 500,
        "stream": False
    }

    try:
        # å‘¼å«æœ¬åœ° LLM
        response = requests.post(LLM_API_URL, json=llm_payload)

        # æª¢æŸ¥å›æ‡‰
        if response.status_code == 200:
            llm_response = response.json()
            ai_reply = llm_response.get("choices", [{}])[0].get("message", {}).get("content", "")
        else:
            print(f"âŒ LLM API éŒ¯èª¤: {response.text}")
            return jsonify({"error": "LLM ç„¡æ³•ç”Ÿæˆ Prompt"}), 500

    except Exception as e:
        print(f"âŒ LLM API é€£ç·šå¤±æ•—: {e}")
        return jsonify({"error": "ç„¡æ³•é€£æ¥åˆ° LLM"}), 500

    print(f"ğŸ¤– AI ç”Ÿæˆçš„ Promptï¼š{ai_reply}")
    return jsonify({"generated_prompt": ai_reply})

# å•Ÿå‹• Flask ä¼ºæœå™¨
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000, debug=True)
